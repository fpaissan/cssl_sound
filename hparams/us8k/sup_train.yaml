# File              : sup_train.yaml
# Author            : Zhepei Wang <zhepeiw2@illinois.edu>
# Date              : 27.01.2022
# Last Modified Date: 30.03.2022
# Last Modified By  : Zhepei Wang <zhepeiw2@illinois.edu>


seed: 2022
__set_seed: !apply:torch.manual_seed [!ref <seed>]
np_rng: !new:numpy.random.RandomState [!ref <seed>]

resume_interrupt: False
resume_task_idx: 0

time_stamp: placeholder
experiment_name: supervise
# output_folder: !ref results/<experiment_name>/<seed>
output_base: !ref /mnt/data/zhepei/outputs/cssl_sound/tdnn_sup
output_folder: !ref <output_base>/<time_stamp>_seed_<seed>+<experiment_name>
train_log: !ref <output_folder>/train_log.txt
save_folder: !ref <output_folder>/save

# cont learning setup
# task_classes:
# - !tuple (0, 1)
# - !tuple (2, 3)
# - !tuple (4, 5)
# - !tuple (6, 7)
# - !tuple (8, 9)
task_classes:
    - !tuple (0, 1, 2, 3, 4, 5, 6, 7, 8, 9)
replay_num_keep: 0

wham_folder: !PLACEHOLDER
wham_audio_folder: !ref <wham_folder>/tr
wham_metadata: "metadata/wham_speechbrain.csv"
add_wham_noise: True

signal_length_s: 4
wham_dataset: !new:dataset.wham_prepare.WHAMDataset
  data_dir: !ref <wham_audio_folder>
  target_length: !ref <signal_length_s>
  sample_rate: !ref <sample_rate>

use_mixup: False
mixup_alpha: 0.4

# Training parameters
number_of_epochs: 50
batch_size: 32
# lr: 0.001
# base_lr: 0.00000001
# max_lr: !ref <lr>
# step_size: 65000
warmup_epochs: 5
warmup_lr: !ref <batch_size> * 0 / 256
base_lr: !ref <batch_size> * 0.03 / 256
final_lr: !ref <batch_size> * 0.00000001 / 256

# dataset
sample_rate: 16000
# train_duration: 4.0
data_folder: "/mnt/data/Sound Sets/UrbanSound8K/UrbanSound8K"
train_folds: [1, 2, 3, 4, 5, 6, 7, 8]
valid_folds: [9]
test_folds: [10]
label_encoder_path: "./dataset/label_encoder_us8k_ordered.txt"
prepare_split_csv_fn: !name:dataset.prepare_urbansound8k.prepare_split_urbansound8k_csv
  root_dir: !ref <data_folder>
  output_dir: !ref <save_folder>
  task_classes: !ref <task_classes>
  train_folds: !ref <train_folds>
  valid_folds: !ref <valid_folds>
  test_folds: !ref <test_folds>

train_dataloader_opts:
  batch_size: !ref <batch_size>
  num_workers: 8
  shuffle: True
  drop_last: True

valid_dataloader_opts:
  batch_size: 32
  num_workers: 8



# Experiment params
auto_mix_prec: True # Set it to True for mixed precision


# Feature parameters
n_mels: 80
left_frames: 0
right_frames: 0
deltas: False
amp_to_db: False
normalize: True
win_length: 25
hop_length: 10
n_fft: !ref <win_length> * <sample_rate> // 1000
f_min: 0
use_time_roll: False
use_freq_shift: False
# Number of classes
n_classes: 10
emb_dim: 2048
emb_norm_type: bn

spec_domain_aug: !new:augmentation.TFAugmentation
    time_warp: True
    time_warp_window: 8
    freq_mask: True
    freq_mask_width: !tuple (0, 10)
    n_freq_mask: 2
    time_mask: True
    time_mask_width: !tuple (0, 10)
    n_time_mask: 2
    replace_with_zero: True
    time_roll: !ref <use_time_roll>
    time_roll_limit: !tuple (0, 200)
    freq_shift: !ref <use_freq_shift>
    freq_shift_limit: !tuple (-10, 10)

# Functions
compute_features: !new:speechbrain.lobes.features.Fbank
    n_mels: !ref <n_mels>
    left_frames: !ref <left_frames>
    right_frames: !ref <right_frames>
    deltas: !ref <deltas>
    sample_rate: !ref <sample_rate>
    n_fft: !ref <n_fft>
    win_length: !ref <win_length>
    hop_length: !ref <hop_length>
    f_min: !ref <f_min>

mean_var_norm: !new:speechbrain.processing.features.InputNormalization
    norm_type: sentence
    std_norm: False

# embedding_model: !new:models.tdnn.ECAPA_TDNN
#     input_size: !ref <n_mels>
#     channels: [1024, 1024, 1024, 1024, 3072]
#     kernel_sizes: [5, 3, 3, 3, 1]
#     dilations: [1, 2, 3, 4, 1]
#     attention_channels: 128
#     lin_neurons: !ref <emb_dim>
embedding_model: !new:models.pann.Cnn14
    mel_bins: !ref <n_mels>
    emb_dim: !ref <emb_dim>
    norm_type: !ref <emb_norm_type>

# classifier: !new:speechbrain.lobes.models.ECAPA_TDNN.Classifier
#     input_size: !ref <emb_dim>
#     out_neurons: !ref <n_classes>
classifier: !new:models.modules.Classifier
    input_size: !ref <emb_dim>
    output_size: !ref <n_classes>

modules:
    compute_features: !ref <compute_features>
    embedding_model: !ref <embedding_model>
    classifier: !ref <classifier>
    mean_var_norm: !ref <mean_var_norm>

compute_cost: !new:losses.LogSoftmaxWithProbWrapper
    loss_fn: !new:torch.nn.Identity
    # loss_fn: !new:speechbrain.nnet.losses.AdditiveAngularMargin
    #     margin: 0.2
    #     scale: 30

acc_metric: !name:speechbrain.utils.Accuracy.AccuracyStats

# opt_class: !name:torch.optim.Adam
#     lr: !ref <base_lr>
#     weight_decay: 0.0005
#
# lr_scheduler_fn: !name:speechbrain.nnet.schedulers.CyclicLRScheduler
#     base_lr: !ref <final_lr>
#     max_lr: !ref <base_lr>
#     step_size: 888
opt_class: !name:torch.optim.SGD
    lr: !ref <base_lr>
    weight_decay: 0
    momentum: 0.9

lr_scheduler_fn: !name:schedulers.SimSiamCosineScheduler
    warmup_epochs: !ref <warmup_epochs>
    warmup_lr: !ref <warmup_lr>
    num_epochs: !ref <number_of_epochs>
    base_lr: !ref <base_lr>
    final_lr: !ref <final_lr>
    steps_per_epoch: 222

epoch_counter_fn: !name:speechbrain.utils.epoch_loop.EpochCounter
    limit: !ref <number_of_epochs>

datapoint_counter: !new:utils.DatapointCounter
      
# # Logging + checkpoints
checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
    checkpoints_dir: !ref <save_folder>
#     recoverables:
#         embedding_model: !ref <embedding_model>
#         classifier: !ref <classifier>
#         normalizer: !ref <mean_var_norm>
#         counter: !ref <epoch_counter>
#         datapoint_counter: !ref <datapoint_counter>
recoverables:
    embedding_model: !ref <embedding_model>
    classifier: !ref <classifier>
    normalizer: !ref <mean_var_norm>
    datapoint_counter: !ref <datapoint_counter>

prev_checkpointer: null
# prev_checkpointer: !new:speechbrain.utils.checkpoints.Checkpointer
#     checkpoints_dir: !PLACEHOLDER

train_logger: !new:speechbrain.utils.train_logger.FileTrainLogger
    save_file: !ref <train_log>

# wandb
use_wandb: False
train_log_frequency: 20
wandb_logger_fn: !name:utils.MyWandBLogger
    initializer: !name:wandb.init
    entity: CAL
    project: cssl_sound
    name: !ref <time_stamp>+seed_<seed>+<experiment_name>
    dir: !ref <output_folder>
    reinit: True
    yaml_config: hparams/us8k/sup_train.yaml
    resume: False

